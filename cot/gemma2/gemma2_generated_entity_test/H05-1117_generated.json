[
    {
        "entity": "automatic evaluation",
        "category": "Task",
        "reasoning": "This describes the overall goal of the research, which is to develop methods for automatic evaluation."
    },
    {
        "entity": "machine translation",
        "category": "Task",
        "reasoning": "This is a specific application domain where automatic evaluation has been developed, providing context for the research."
    },
    {
        "entity": "document summarization",
        "category": "Task",
        "reasoning": "Another specific application domain where automatic evaluation has been developed, providing context for the research."
    },
    {
        "entity": "POURPRE",
        "category": "Method",
        "reasoning": "This is the name of the specific metric developed in this research for automatic evaluation of answers to definition questions."
    },
    {
        "entity": "answers to definition questions",
        "category": "Task",
        "reasoning": "This is the specific type of task that the POURPRE metric is designed to evaluate."
    },
    {
        "entity": "scoring system output",
        "category": "Task",
        "reasoning": "This describes the specific goal of developing automatic methods, which is to score the output of systems."
    },
    {
        "entity": "TREC 2003 and TREC 2004 QA tracks",
        "category": "Material",
        "reasoning": "These are specific datasets or benchmarks used in the experiments to evaluate the POURPRE metric."
    },
    {
        "entity": "rankings",
        "category": "Metric",
        "reasoning": "This is a measure of performance used to compare different systems or metrics."
    },
    {
        "entity": "official rankings",
        "category": "Metric",
        "reasoning": "This refers to the ground truth or standard rankings used as a benchmark for comparison."
    }
]